{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "COM4509_6509_Assignment_Part1_Username_act21sb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G653UFRLjPjK"
      },
      "source": [
        "# Assignment Part 1 Brief \n",
        "\n",
        "## Deadline: Friday, December 3, 2021 at 15:00 hrs\n",
        "\n",
        "## Number of marks available for Part 1: 25\n",
        "\n",
        "## Scope: Sessions 1 to 5\n",
        "\n",
        "### Please READ the whole assignment first, before starting to work on it.\n",
        "\n",
        "### How and what to submit\n",
        "\n",
        "A. A **Jupyter Notebook** with the code in all the cells executed and outputs displayed.\n",
        "\n",
        "B. Name your Notebook **COM4509-6509_Assignment_Part1_Username_XXXXXX.ipynb** where XXXXXX is your username such as such as abc18de.  \n",
        "\n",
        "C. Upload a .zip file to Blackboard before the deadline that contains the Jupyter Notebook in B and any other files requested for the solution of Part 2 of the Assignment (Dr Lu will be in charge of releasing this part). \n",
        "\n",
        "D. **NO DATA UPLOAD**: Please do not upload the data files used in this Notebook. We have a copy already. \n",
        "\n",
        "\n",
        "### Assessment Criteria \n",
        "\n",
        "* Being able to use numpy and pandas to preprocess a dataset.\n",
        "\n",
        "* Being able to use numpy to build a machine learning pipeline for supervised learning. \n",
        "\n",
        "* Being able to follow the steps involved in an end-to-end project in machine learning.\n",
        "\n",
        "* Being able to use scikit-learn to design a machine learning model pipeline\n",
        "\n",
        "\n",
        "### Late submissions\n",
        "\n",
        "We follow Department's guidelines about late submissions, i.e., a deduction of 5% of the mark each working day the work is late after the deadline. NO late submission will be marked one week after the deadline because we will release a solution by then. Please read [this link](https://sites.google.com/sheffield.ac.uk/comughandbook/general-information/assessment/late-submission) if you are taking COM4509 or read [this link](https://sites.google.com/sheffield.ac.uk/compgtstudenthandbook/menu/assessment/late-submission) if you are taking COM6509. \n",
        "\n",
        "### Use of unfair means \n",
        "\n",
        "**\"Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.\"** (from the students Handbook). Please carefully read [this link](https://sites.google.com/sheffield.ac.uk/comughandbook/general-information/assessment/unfair-means) on what constitutes Unfair Means if not sure, for COM4509. If you are taking COM6509, please read [this link](https://sites.google.com/sheffield.ac.uk/compgtstudenthandbook/menu/referencing-unfair-means) if you are not sure what is Unfair means. If you still have questions, please ask your Personal tutor or the Lecturers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnq7Vv8IjPjS"
      },
      "source": [
        "# A dataset of air quality\n",
        "\n",
        "The dataset you will use in this assignment comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). We are going to use regularised ridge regression and random forests for predicting air quality. The description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Air+Quality)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwHMPTVBjPjT",
        "outputId": "abde7d6e-5a90-49ce-c161-33b904594513"
      },
      "source": [
        "import urllib.request\n",
        "doq = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip\"\n",
        "pat_sav = \"./AirQualityUCI.zip\"\n",
        "urllib.request.urlretrieve(doq, pat_sav)\n",
        "#urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./AirQualityUCI.zip', <http.client.HTTPMessage at 0x7fa720bf9210>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSRmspy1jPjW"
      },
      "source": [
        "import zipfile\n",
        "zip = zipfile.ZipFile('./AirQualityUCI.zip', 'r')\n",
        "for name in zip.namelist():\n",
        "    zip.extract(name, '.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orgr6e4rjPjW"
      },
      "source": [
        "# The .csv version of the file has some typing issues, so we use the excel version\n",
        "import pandas as pd \n",
        "air_quality_full = pd.read_excel('./AirQualityUCI.xlsx', usecols=range(2,15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6DoPgmojPjX"
      },
      "source": [
        "We can see some of the rows in the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XM7iz27ajPjX",
        "outputId": "5409f959-fced-4274-8f03-6ac53a1541cd"
      },
      "source": [
        "air_quality_full.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1894</th>\n",
              "      <td>1.7</td>\n",
              "      <td>1039.25</td>\n",
              "      <td>-200</td>\n",
              "      <td>12.156068</td>\n",
              "      <td>1055.00</td>\n",
              "      <td>131.0</td>\n",
              "      <td>965.50</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1589.5</td>\n",
              "      <td>844.50</td>\n",
              "      <td>27.975000</td>\n",
              "      <td>23.175000</td>\n",
              "      <td>0.861267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3462</th>\n",
              "      <td>1.9</td>\n",
              "      <td>1141.75</td>\n",
              "      <td>-200</td>\n",
              "      <td>9.132252</td>\n",
              "      <td>944.50</td>\n",
              "      <td>65.0</td>\n",
              "      <td>671.75</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1774.5</td>\n",
              "      <td>798.50</td>\n",
              "      <td>27.324999</td>\n",
              "      <td>57.075000</td>\n",
              "      <td>2.042587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6532</th>\n",
              "      <td>2.4</td>\n",
              "      <td>1194.50</td>\n",
              "      <td>-200</td>\n",
              "      <td>10.963458</td>\n",
              "      <td>1013.00</td>\n",
              "      <td>355.0</td>\n",
              "      <td>645.75</td>\n",
              "      <td>104.0</td>\n",
              "      <td>1344.5</td>\n",
              "      <td>1275.75</td>\n",
              "      <td>14.475000</td>\n",
              "      <td>62.325001</td>\n",
              "      <td>1.021644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2363</th>\n",
              "      <td>0.5</td>\n",
              "      <td>797.50</td>\n",
              "      <td>-200</td>\n",
              "      <td>3.402194</td>\n",
              "      <td>676.50</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1091.50</td>\n",
              "      <td>44.0</td>\n",
              "      <td>1473.0</td>\n",
              "      <td>696.00</td>\n",
              "      <td>20.175000</td>\n",
              "      <td>53.775001</td>\n",
              "      <td>1.256906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4895</th>\n",
              "      <td>6.5</td>\n",
              "      <td>1728.00</td>\n",
              "      <td>-200</td>\n",
              "      <td>35.871083</td>\n",
              "      <td>1683.25</td>\n",
              "      <td>623.0</td>\n",
              "      <td>418.50</td>\n",
              "      <td>188.0</td>\n",
              "      <td>2365.0</td>\n",
              "      <td>1861.00</td>\n",
              "      <td>25.425000</td>\n",
              "      <td>47.925000</td>\n",
              "      <td>1.534608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CO(GT)  PT08.S1(CO)  NMHC(GT)  ...          T         RH        AH\n",
              "1894     1.7      1039.25      -200  ...  27.975000  23.175000  0.861267\n",
              "3462     1.9      1141.75      -200  ...  27.324999  57.075000  2.042587\n",
              "6532     2.4      1194.50      -200  ...  14.475000  62.325001  1.021644\n",
              "2363     0.5       797.50      -200  ...  20.175000  53.775001  1.256906\n",
              "4895     6.5      1728.00      -200  ...  25.425000  47.925000  1.534608\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBFFKYFvjPjY"
      },
      "source": [
        "The target variable corresponds to the CO(GT) variable of the first column. The following columns correspond to the variables in the feature vectors, *e.g.*, PT08.S1(CO) is $x_1$ up until AH which is $x_D$. The original dataset also has a date and a time columns that we are not going to use in this assignment.\n",
        "\n",
        "### Removing instances \n",
        "\n",
        "The dataset has missing values tagged with a -200 value. To simplify the design of the machine learning models in this assignment, we perform the following two operations to the dataset right from the beginning:\n",
        "\n",
        "* we will remove the rows for which the target variable has missing values. We are doing supervised learning so we need all our data observations to have known target values.\n",
        "\n",
        "* we will remove features with more than 20% of missing values. \n",
        "\n",
        "The code below performs both operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av_CpbIBjPja"
      },
      "source": [
        "# We first remove the rows for which there are missing values in the target feature\n",
        "air_quality = air_quality_full.loc[air_quality_full.iloc[:, 0]!=-200, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXaYHJlpjPjb"
      },
      "source": [
        "# We now remove the columns (features) for which there are more that 20% of missing values\n",
        "import numpy as np\n",
        "ndata, ncols = np.shape(air_quality) # number of data observations and number of columns in the dataframe\n",
        "pmissing = np.empty(ncols)         # An empty vector that will keep the percentage of missing values per feature\n",
        "for i in range(ncols):\n",
        "    pmissing[i] = (air_quality.iloc[:, i]==-200).sum()/ndata # Computes the percentage of missing values per column\n",
        "air_quality = air_quality.loc[:, pmissing < 0.2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWDzeBK2jPjc"
      },
      "source": [
        "### Splitting the dataset \n",
        "\n",
        "Before designing any machine learning model, we need to set aside the test data. We will use the remaining training data for fitting the model. *It is important to remember that the test data has to be set aside before preprocessing*. \n",
        "\n",
        "Any preprocessing that you do has to be done only on the training data and several key statistics need to be saved for the test stage.  Separating the dataset into training and test before any preprocessing has happened help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
        "\n",
        "Furthermore, we are going to use *hold-out validation* for validating our predictive model so we need to further separate the training data into a training set and a validation set.\n",
        "\n",
        "We split the dataset into a training set, a validation set and a test set. The training set will have 70% of the total observations, the validation set 15% and the test set, the remaining 15%. For making the random selections of the training and validation sets **make sure that you use a random seed that corresponds to the last five digits of your student UCard**. In the code below, I have used 55555 as an example of my random seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q86-nfPzjPjd"
      },
      "source": [
        "np.random.seed(68744)                 # Make sure you use the last five digits of your student UCard as your seed\n",
        "index = np.random.permutation(ndata)  # We permute the indexes \n",
        "N = np.int64(np.round(0.70*ndata))    # We compute N, the number of training instances\n",
        "Nval = np.int64(np.round(0.15*ndata)) # We compute Nval, the number of validation instances   \n",
        "Ntest = ndata - N - Nval              # We compute Ntest, the number of test instances\n",
        "data_training_unproc = air_quality.iloc[index[0:N], :].copy() # Select the training data\n",
        "data_val_unproc = air_quality.iloc[index[N:N+Nval], :].copy() # Select the validation data\n",
        "data_test_unproc = air_quality.iloc[index[N+Nval:ndata], :].copy() # Select the test data\n",
        "#68744 u card"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFINQ0lYjPje"
      },
      "source": [
        "The assigment is divided into two sections. In the **first section**, you will design a regularised ridge regression model trained with stochastic gradient descent. You will write all the code from scratch. You should not use any library that already implements any of the routines considered in this section, for example, scikit-learn. In the **second section**, you will design a random forests model and you are allowed to use scikit-learn in this section.\n",
        "\n",
        "When writing your code, you will find out that there are operations that are repeated at least twice. We will assign marks for use of Python functions and for commenting your code. The marks will be assigned as:\n",
        "\n",
        "* Did you include Python functions to solve the question and avoid repeating code? (**1 mark**)\n",
        "* Did you comment your code to make it readable to others? (**1 mark**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pMSJzYojPjf"
      },
      "source": [
        "# 1. Using regularised ridge regression to predict air quality (10 marks)\n",
        "\n",
        "**DO NOT USE scikit-learn or any other machine learning library for the questions on this section. You are meant to write Python code from scratch. You can use Pandas and Numpy. Using scikit-learn or any other machine learning library for the questions in this section will give ZERO marks. No excuse will be accepted.**\n",
        "\n",
        "Regularisation is a technique commonly used in Machine Learning to prevent overfitting. It consists on adding terms to the objective function such that the optimisation procedure avoids solutions that just learn the training data. Popular techniques for regularisation in Supervised Learning include [Lasso Regression](https://en.wikipedia.org/wiki/Lasso_(statistics)), [Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) and the [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization). \n",
        "\n",
        "In this part of the Assignment, you will be looking at Ridge Regression and implementing equations to optimise the objective function using the update rules for stochastic gradient descent. You will use those update rules for making predictions on the Air Quality dataset.\n",
        "\n",
        "## 1.1 Ridge Regression\n",
        "\n",
        "Let us start with a data set for training $\\mathcal{D} = \\{\\mathbf{y}, \\mathbf{X}\\}$, where the vector $\\mathbf{y}=[y_1, \\cdots, y_N]^{\\top}$ and $\\mathbf{X}$ is the design matrix from Lab 4, this is, \n",
        "\n",
        "\\begin{align*}\n",
        "    \\mathbf{X} = \n",
        "                \\begin{bmatrix}\n",
        "                        1 & x_{1,1} & \\cdots & x_{1, D}\\\\\n",
        "                        1 & x_{2,1} & \\cdots & x_{2, D}\\\\\n",
        "                   \\vdots &  \\vdots\\\\\n",
        "                        1 & x_{N,1} & \\cdots & x_{N, D}\n",
        "                \\end{bmatrix}\n",
        "               = \n",
        "               \\begin{bmatrix}\n",
        "                      \\mathbf{x}_1^{\\top}\\\\\n",
        "                       \\mathbf{x}_2^{\\top}\\\\\n",
        "                          \\vdots\\\\\n",
        "                        \\mathbf{x}_N^{\\top}\n",
        "                \\end{bmatrix}.\n",
        "\\end{align*}\n",
        "\n",
        "Our predictive model is going to be a linear model\n",
        "\n",
        "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
        "\n",
        "where $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$.\n",
        "\n",
        "The **objective function** we are going to use has the following form\n",
        "\n",
        "$$ E(\\mathbf{w}, \\lambda) = \\frac{1}{N}\\sum_{n=1}^N (y_n - f(\\mathbf{x}_n))^2 + \\frac{\\lambda}{2}\\sum_{j=0}^D w_j^2,$$\n",
        "\n",
        "where $\\lambda>0$ is known as the *regularisation* parameter.\n",
        "\n",
        "This objective function was studied in Lecture 4. \n",
        "\n",
        "The first term on the rhs is what we call the \"fitting\" term whereas the second term in the expression is the regularisation term. Given $\\lambda$, the two terms in the expression have different purposes. The first term is looking for a value of $\\mathbf{w}$ that leads the squared-errors to zero. While doing this, $\\mathbf{w}$ can take any value and lead to a solution that it is only good for the training data but perhaps not for the test data. The second term is regularising the behavior of the first term by driving the $\\mathbf{w}$ towards zero. By doing this, it restricts the possible set of values that $\\mathbf{w}$ might take according to the first term. The value that we use for $\\lambda$ will allow a compromise between a value of $\\mathbf{w}$ that exactly fits the data (first term) or a value of $\\mathbf{w}$ that does not grow too much (second term).\n",
        "\n",
        "This type of regularisation has different names: ridge regression, Tikhonov regularisation or $\\ell_2$ norm regularisation. \n",
        "\n",
        "## 1.2 Optimising the objective function with respect to $\\mathbf{w}$\n",
        "\n",
        "There are two ways we can optimise the objective function with respect to $\\mathbf{w}$. The first one leads to a closed form expression for $\\mathbf{w}$ and the second one using an iterative optimisation procedure that updates the value of $\\mathbf{w}$ at each iteration by using the gradient of the objective function with respect to $\\mathbf{w}$,\n",
        "$$\n",
        "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}},\n",
        "$$\n",
        "where $\\eta$ is the *learning rate* parameter and $\\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}}$ is the gradient of the objective function.\n",
        "\n",
        "It can be shown (this is a question in the Exercise Sheet 4) that a closed-form expression for the optimal $\\mathbf{w}_*$ is given as\n",
        "\n",
        "\\begin{align*}            \n",
        "            \\mathbf{w}_*& = \\left(\\mathbf{X}^{\\top}\\mathbf{X} + \\frac{\\lambda N}   \n",
        "                                     {2}\\mathbf{I}\\right)^{-1}\\mathbf{X}^{\\top}\\mathbf{y}.\n",
        "\\end{align*}\n",
        "\n",
        "Alternatively, we can find an update equation for $\\mathbf{w}_{\\text{new}}$ using gradient descent leading to:\n",
        "\n",
        "\\begin{align*}\n",
        "   \\mathbf{w}_{\\text{new}} & = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}\n",
        "                              {d\\mathbf{w}},\\\\\n",
        "                           & = \\mathbf{w}_{\\text{old}} +  \\frac{2\\eta}{N}\\sum_{n=1}^N   \n",
        "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n  \n",
        "                       - \\eta\\lambda\\mathbf{w}_{\\text{old}}\\\\\n",
        "                           & = (1 - \\eta\\lambda)\\mathbf{w}_{\\text{old}} + \\frac{2\\eta}\n",
        "                               {N}\\sum_{n=1}^N   \n",
        "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-rfm1WfjPjf"
      },
      "source": [
        "## 1.3 Preprocessing the data\n",
        "\n",
        "As mentioned before, the dataset has missing values tagged with a -200 value. Before doing any work with the training data, we want to make sure that we deal properly with the missing values. Furthermore, once we have dealt with the missing values, we want to standardise the training data. \n",
        "\n",
        "### Question 1.a: Missing values and standardisation (2 marks)\n",
        "\n",
        "* For all the other features with missing values, use the mean value of the non-missing values for that feature to perform imputation. Save these mean values, you will need them when performing the validation stage (**1 mark**).\n",
        "\n",
        "* Once you have imputed the missing data, we need to standardise the input vectors. Standardise the training data by substracting the mean value for each feature and dividing the result by the standard deviation of each feature. Keep the mean values and standard deviations, you will need them at validation time (**1 mark**).\n",
        "\n",
        "#### Question 1.a Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DoKV0chdjPjf",
        "outputId": "9cafcf9e-fed6-48ea-dab5-42fdd048e27b"
      },
      "source": [
        "pl_hld = pd.DataFrame(columns = data_training_unproc.columns)\n",
        "\n",
        "for i in data_training_unproc:\n",
        "  # Need a temporary data set with all the non-missing values from the training dataset\n",
        "  plhld_var = data_training_unproc[data_training_unproc[i]!=-200]\n",
        "  pl_hld[i] = plhld_var[i]\n",
        "\n",
        "#Calculate the mean of all values inside the placeholder\n",
        "cmean = pl_hld.mean()\n",
        "\n",
        "\n",
        "#imputation\n",
        "# overwrite the missing values in the training dataset with the means\n",
        "train_replace = data_training_unproc.replace(-200, cmean)\n",
        "\n",
        "#Calculate the standard deviation of every value inside the training dataset\n",
        "standard_dev = train_replace.std()\n",
        "\n",
        "#STANDARDIZATION of training data\n",
        "train_dataset = (train_replace - cmean)/standard_dev\n",
        "\n",
        "\n",
        "#SANITY CHECK# Make sure that the notebook displays the plot properly\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot\n",
        "pyplot.style.use('dark_background')\n",
        "train_dataset.hist(bins=30, figsize=(15, 13))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa720288810>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa72005aa90>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa7206b40d0>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fa71fdc26d0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa71fd81cd0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa71fd07310>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fa71fe80990>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa71fd37ed0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa71fd37f10>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fa71fcbe650>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa71fdf8190>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa71fc70790>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x936 with 12 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7DDUQOFjPjg"
      },
      "source": [
        "## 1.4 Training and validation stages\n",
        "\n",
        "We have now curated our training data by removing data observations and features with a large amount of missing values. We have also normalised the feature vectors. We are now in a good position to work on developing the prediction model and validating it. We will use gradient descent for iterative optimisation. \n",
        "\n",
        "We first organise the dataframe into the vector of targets $\\mathbf{y}$, call it `yTrain`, and the design matrix $\\mathbf{X}$, call it `XTrain`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Sdur68jPjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8bd24c-a87c-47e2-f411-8106bfee9c6a"
      },
      "source": [
        "#Training Dataset separated into design model xTrain, from column PT08.S1(CO) all the way to the last column (AH)\n",
        "xTrain = pd.DataFrame(train_dataset.iloc[:, 1:train_dataset.last_valid_index()])\n",
        "\n",
        "#For yTrain we need to reverse the standardization, otherwise it will affect the computation of w, using minibatch gradient descent.\n",
        "NoStan_train = train_dataset*standard_dev\n",
        "\n",
        "\n",
        "#Training Dataset separated into the vector of targets y, called yTrain. take the first column CO(GT)\n",
        "yTrain = pd.DataFrame(NoStan_train.iloc[:, 0])\n",
        "\n",
        "\n",
        "print(xTrain)\n",
        "print(yTrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  ...         T        RH        AH\n",
            "1097    -1.165636 -1.032958      -1.201617  ... -0.060000 -0.829593 -0.787680\n",
            "2756    -1.006900 -0.570714      -0.500915  ...  2.261076 -1.947816  0.032375\n",
            "3479    -0.287917  0.358418       0.512274  ...  2.198034 -1.548763  1.026382\n",
            "6798     0.483589  0.720063       0.840099  ... -1.128841 -0.594254 -1.461240\n",
            "3764    -1.310367 -0.943068      -1.044414  ...  1.960196 -1.443519  0.895207\n",
            "...           ...       ...            ...  ...       ...       ...       ...\n",
            "9344    -0.933367 -1.201460      -1.549571  ... -0.641702  0.418724 -0.496819\n",
            "5636    -0.234226 -0.378473      -0.262236  ...  0.601936  0.965411  2.115502\n",
            "8767    -0.395297 -0.755668      -0.753973  ... -1.243462 -0.250748 -1.380082\n",
            "4574    -0.353279 -0.089026       0.063672  ...  0.229418  0.503504  0.828326\n",
            "6776    -0.119843 -0.709240      -0.687833  ... -0.610181  1.440472  0.179590\n",
            "\n",
            "[5372 rows x 11 columns]\n",
            "        CO(GT)\n",
            "1097 -1.259661\n",
            "2756 -1.359661\n",
            "3479 -0.259661\n",
            "6798  1.440339\n",
            "3764 -1.459661\n",
            "...        ...\n",
            "9344 -1.659661\n",
            "5636 -0.659661\n",
            "8767 -0.759661\n",
            "4574  0.040339\n",
            "6776 -0.759661\n",
            "\n",
            "[5372 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvp6k3CNjPjh"
      },
      "source": [
        "### Question 1.b: finding the optimal $\\mathbf{w}$ with stochastic gradient descent (3 marks)\n",
        "\n",
        "Use gradient descent to iteratively compute the value of $\\mathbf{w}_{\\text{new}}$. Instead of using all the training set in `XTrain` and `yTrain` to compute the gradient, use a subset of $S$ instances in `XTrain` and `yTrain`. This is sometimes called *minibatch gradient descent* where $S$ is the size of the minibacth. When using gradient descent with minibatches, you need to find the best values for three parameters: $\\eta$, the learning rate, $S$, the number of datapoints in the minibatch and $\\gamma$, the regularisation parameter.\n",
        "\n",
        "* In this question we will use the validation data. So before proceeding to the next steps, make sure that you:  replace the missing values on each feature variables with the mean value you computed with the training data; standardise the validation data using the means and standard deviations computed from the training data (**1 mark**).\n",
        "    \n",
        "* Create a grid of values for the parameters $\\gamma$ and $\\eta$ using `np.logspace` and a grid of values for $S$ using `np.linspace`. Because you need to find three parameters, start with five values for each parameter in the grid and see if you can increase it. Make sure you understand what is the meaning of `np.logspace` and `np.linspace`. Notice that you can use negative values for `start` in `np.logspace` (**1 mark**).\n",
        "\n",
        "* For each value that you have of $\\gamma$, $\\eta$ and $S$ from the previous step, use the training set to compute $\\mathbf{w}$ using minibatch gradient descent and then measure the RMSE over the validation data. For the validation data, make sure you preprocess it before applying the prediction model over it. For the minibatch gradient descent choose to stop the iterative procedure after $200$ iterations (**1 mark**).\n",
        "\n",
        "* Choose the values of $\\gamma$, $\\eta$ and $S$ that lead to the lowest RMSE and save them. You will use them at the test stage.\n",
        "\n",
        "#### Question 1.b Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk13cl0NjPji"
      },
      "source": [
        "# replace the missing values on each feature variables with the mean value you computed with the training data\n",
        "\n",
        "#standardise the validation data using the means and standard deviations computed from the training data\n",
        "\n",
        "#VALIDATION DATA\n",
        "val_replace = data_val_unproc.replace(-200, cmean)\n",
        "\n",
        "Val_dataset = (val_replace - cmean)/standard_dev\n",
        "\n",
        "#Validation Dataset separated into design model xTrain, from column PT08.S1(CO) all the way to the last column (AH)\n",
        "xVal = pd.DataFrame(Val_dataset.iloc[:, 1:Val_dataset.last_valid_index()])\n",
        "\n",
        "#For yVal we need to reverse the standardization, otherwise it will affect the computation of w, using minibatch gradient descent.\n",
        "NoStan_Val = Val_dataset*standard_dev\n",
        "\n",
        "#Validation Dataset separated into the vector of targets y, called yTrain. take the first column CO(GT)\n",
        "yVal = pd.DataFrame(NoStan_Val.iloc[:, 0])\n",
        "\n",
        "\n",
        "\n",
        "#find 3 param γ, η and S, each must have at least 5 in the grid\n",
        "\n",
        "  #np.logspace => γ, η\n",
        "γ_log = np.logspace(-13.0,\n",
        "               -12.0,\n",
        "               num = 5,\n",
        "               endpoint = True,\n",
        "               base = 2.0,\n",
        "               dtype = None)\n",
        "\n",
        "print(\"γ_log\\n\", γ_log,'\\n')\n",
        "\n",
        "# 3.0,               4.0,\n",
        "η_log = np.logspace(-8.0,\n",
        "                    -7.0,\n",
        "               num = 5,\n",
        "               endpoint = True,\n",
        "               base = 2.0,\n",
        "               dtype = None)\n",
        "\n",
        "print(\"η_log\\n\", η_log, '\\n')\n",
        "\n",
        "  #np.linspace => S\n",
        "\n",
        "S_lin = np.linspace(95,\n",
        "                    100,\n",
        "               num = 5,\n",
        "               endpoint = True,\n",
        "               retstep = False,\n",
        "               dtype = int)\n",
        "\n",
        "print('S_lin\\n', S_lin)\n",
        "\n",
        "#S is the size of the minibacth.\n",
        "# When using gradient descent with minibatches, you need to find the best values for three parameters:\n",
        " # η, the learning rate, \n",
        " # S, the number of datapoints in the minibatch \n",
        " # and γ, the regularisation parameter.\n",
        "\n",
        "\n",
        "# linear regression using \"mini-batch\" gradient descent\n",
        "# Compute hypothesis / predictions\n",
        "def hypothesis(X, theta):\n",
        "    return np.dot(X, theta)\n",
        "\n",
        "  \n",
        "# Compute gradient of error function w.r.t. theta\n",
        "def gradient(X, y, theta):\n",
        "    h = hypothesis(X, theta)\n",
        "    grad = np.dot(X.transpose(), (h - y))\n",
        "   \n",
        "    return grad\n",
        "\n",
        "#Cost function for ridge regression \n",
        "def cost_Reg(X,y,theta,lmd):\n",
        "    m = len(y) \n",
        "    h = hypothesis(X, theta)\n",
        "    J_reg = (lmd / (2*m)) * np.sum(np.square(theta))\n",
        "    J = float((1./(2*m)) * np.dot((h - y).transpose(), (h - y))) + J_reg\n",
        "    \n",
        "    return J \n",
        "\n",
        "#Create a list containing randomized mini-batches\n",
        "# For the minibatch gradient descent choose to stop the iterative procedure after 200 iterations\n",
        "def create_mini_batches(X, y, batch_size):\n",
        "    mini_batches = []\n",
        "    data = np.hstack((X, y))\n",
        "    np.random.shuffle(data)\n",
        "    n_minibatches = data.shape[0] // batch_size\n",
        "    i = 0\n",
        "  \n",
        "    for i in range(n_minibatches + 1):\n",
        "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]\n",
        "        X_mini = mini_batch[:, :-1]\n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
        "        mini_batches.append((X_mini, Y_mini))\n",
        "    if data.shape[0] % batch_size != 0:\n",
        "        mini_batch = data[i * batch_size:data.shape[0]]\n",
        "        X_mini = mini_batch[:, :-1]\n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
        "        mini_batches.append((X_mini, Y_mini))\n",
        "    return mini_batches\n",
        "   \n",
        "  \n",
        "#Calculate mini-batch Gradient descent for ridge regression\n",
        "def gradient_descent_reg(X,y,learning_rate,lmd, batch_size):\n",
        "    m = np.size(y)\n",
        "    theta = np.zeros((X.shape[1], 1))\n",
        "    nr_iters = 200\n",
        "    errors = []\n",
        "\n",
        "    for itr in range(nr_iters):\n",
        "        mini_batches = create_mini_batches(X, y, batch_size)\n",
        "\n",
        "        for mini_batch in mini_batches:\n",
        "            X_mini, y_mini = mini_batch\n",
        "            theta = theta - learning_rate *(1/m)* (gradient(X, y, theta) + lmd * theta) \n",
        "            errors.append(cost_Reg(X_mini, y_mini, theta, lmd))\n",
        "            \n",
        "    return theta ,errors\n",
        "\n",
        "\n",
        "#Replace learning rate n and batch size S for every version and the other variables\n",
        "\n",
        "\n",
        "\n",
        " # η, is the learning rate, \n",
        " # S, the number of datapoints in the minibatch (batch size) \n",
        " # and γ, the regularisation parameter.\n",
        "\n",
        "# Will need to use this again, better to have it defined as a function \n",
        "def Find_RMSE(x,y, x_v, y_v):\n",
        "  RMSE = 10000\n",
        "#Loop this for all batch_size,learning_rate ,lmd values\n",
        "#make a function for later use\n",
        "  for j in η_log:\n",
        "    for t in S_lin:\n",
        "      for i in γ_log:\n",
        "\n",
        "\n",
        "        print('\\nη_log is: ', j,'\\nS_lin is: ', t,'\\nγ_log is: ', i)\n",
        "        # learning_rate = j\n",
        "        # batch_size = t\n",
        "        # lambda = i\n",
        "      \n",
        "        #Performs L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients\n",
        "        theta, errors = gradient_descent_reg(x, y,j, i,t)\n",
        "\n",
        "\n",
        "        print(\"\\nBias = \", theta[0])\n",
        "        print(\"\\nCoefficients = \", theta[1:])\n",
        "\n",
        "    \n",
        "        y_pred = hypothesis(x_v, theta)\n",
        "\n",
        "\n",
        "        #Compute the w of the ridge regression, using the training set\n",
        "     \n",
        "        n, m = x.shape\n",
        "        I = np.identity(m)\n",
        "        w = np.dot(np.dot(np.linalg.inv(np.dot(x.transpose(), x) + i * I), x.transpose()), y)\n",
        "\n",
        "        print('\\nw is :',w)\n",
        "\n",
        "        # Measure the RMSE over the validation data\n",
        "        temp_RMSE = float(np.sqrt(np.mean((y_v   - y_pred)**2)))\n",
        "\n",
        "        #Store the smallest value of RMSE and save the \n",
        "        if RMSE > temp_RMSE:\n",
        "          RMSE = temp_RMSE\n",
        "          γ_log_sol = i\n",
        "          η_log_sol = j\n",
        "          S_lin_sol = t\n",
        "\n",
        "          print('\\nTemporary finds: ', RMSE,' ', γ_log_sol, ' ',η_log_sol, ' ',S_lin_sol)\n",
        "\n",
        "  return RMSE,γ_log_sol, η_log_sol, S_lin_sol\n",
        "\n",
        "#IV#\n",
        "#Choose the values of γ, η and S that lead to the lowest RMSE and save them. You will use them at the test stage.\n",
        "\n",
        "RMSE,γ_log_sol,η_log_sol,S_lin_sol= Find_RMSE(xTrain, yTrain, xVal, yVal)\n",
        "\n",
        "print(\"\\nRoot Mean Square Error = \", RMSE)\n",
        "print('\\nVariable η_log is: ', η_log_sol,'\\nnVariable S_lin is: ', S_lin_sol,'\\nnVariable γ_log is: ', γ_log_sol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jaOW3qcjPji"
      },
      "source": [
        "## 1.5 Test stage \n",
        "\n",
        "We now know which one is the best model, according to the validation data. We will now put together the training data and the validation data, perform the preprocessing as we did before, this is, treat the missing values and standardise the inputs. We will train the model again using the minibatch stochastic gradient descent and finally compute the RMSE over the test data.\n",
        "\n",
        "\n",
        "### Question 1.c: combine the original training and original validation data and perform the preprocessing again to this new data (2 marks)\n",
        "\n",
        "Put together the original training and validation dataset and perform the same preprocessing steps than before, these are: \n",
        "\n",
        "* for each feature, impute the missing values with the mean values of the non-missing values (**1 mark**) \n",
        "\n",
        "* stardardise the new training set (**1 mark**).\n",
        "\n",
        "#### Question 1.c Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-2O8oJ5jPji"
      },
      "source": [
        "\n",
        "\n",
        "# Training_unprocesed + Validation_unprocesed dataset\n",
        "new_data = [data_val_unproc, data_training_unproc]\n",
        "  \n",
        "new_unproc_train = pd.concat(new_data)\n",
        "display(new_unproc_train)\n",
        "\n",
        "#Calculate the means of the non-missing values\n",
        "pl_hld_2 = pd.DataFrame(columns = new_unproc_train.columns)\n",
        "\n",
        "for i in new_unproc_train:\n",
        "  # Need a temporary data set with all the non-missing values from the new training dataset\n",
        "  plhld_var_2 = new_unproc_train[new_unproc_train[i]!=-200]\n",
        "  pl_hld_2[i] = plhld_var_2[i]\n",
        "\n",
        "#Calculate the mean of all values inside the placeholder\n",
        "new_mean = pl_hld_2.mean()\n",
        "\n",
        "#Calculate the standard deviation\n",
        "standard_dev_2 = new_unproc_train.std()\n",
        "\n",
        "#Replace missing values with the mean values of the non-missing values\n",
        "new_replace = new_unproc_train.replace(-200, new_mean)\n",
        "\n",
        "#Standardise the new training set\n",
        "new_train_dataset = (new_replace - new_mean)/standard_dev_2\n",
        "\n",
        "\n",
        "#New Testing Dataset separated into the vector of targets y, called x_newTrain. take the first column CO(GT)\n",
        "x_newTrain = pd.DataFrame(new_train_dataset.iloc[:, 1:new_train_dataset.last_valid_index()])\n",
        "\n",
        "#New Testing Dataset separated into the vector of targets y, called y_newTrain. take the first column CO(GT)\n",
        "y_newTrain = pd.DataFrame(new_train_dataset.iloc[:, 0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4DB_F8PjPjj"
      },
      "source": [
        "### Question 1.d: Preprocess the test data, train the model and predict over the test data (3 marks)\n",
        "\n",
        "Preprocess the test data and train a new model using the new training set. Finally, report the RMSE over the test set:\n",
        "\n",
        "* Preprocess the test data by imputing the missing data and standardising it (**1 mark**). \n",
        "\n",
        "* Use the best values of $\\gamma$, $\\eta$ and $S$ found in the validation set and train a new regularised linear model with stochastic gradient descent (**1 mark**).\n",
        "\n",
        "* Report the RMSE over the test data (**1 mark**).\n",
        "\n",
        "#### Question 1.d Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpyzoCYSjPjj"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "# Preprocess TEST DATA\n",
        "\n",
        "#Calculate the means of the non-missing values\n",
        "pl_hld_3 = pd.DataFrame(columns = data_test_unproc.columns)\n",
        "\n",
        "for i in new_unproc_train:\n",
        "  # Need a temporary data set with all the non-missing values from the test dataset\n",
        "  plhld_var_3 = data_test_unproc[data_test_unproc[i]!=-200]\n",
        "  pl_hld_3[i] = plhld_var_3[i]\n",
        "\n",
        "#Calculate the mean of all values inside the placeholder\n",
        "tmean = pl_hld_3.mean()\n",
        "\n",
        "#Calculate the standard deviation\n",
        "standard_dev_3 = data_test_unproc.std()\n",
        "\n",
        "#Replace missing values with the mean values of the non-missing values\n",
        "data_test_unproc = data_test_unproc.replace(-200, tmean)\n",
        "\n",
        "#Standardise the new testing set\n",
        "TST_dataset = (data_test_unproc - tmean)/standard_dev_3\n",
        "\n",
        "\n",
        " x_Test = pd.DataFrame(TST_dataset.iloc[:, 1:TST_dataset.last_valid_index()])\n",
        "\n",
        "# New Testing Dataset separated into the vector of targets y, called y_newTrain. take the first column CO(GT)\n",
        " y_Test = pd.DataFrame(TST_dataset.iloc[:, 0])\n",
        "\n",
        "\n",
        "#Preprocess the test data and train a new model using the new training set\n",
        "\n",
        "\n",
        "#apply method\n",
        "\n",
        "\n",
        "\n",
        "new_RMSE,new_γ_log_sol,new_η_log_sol,new_S_lin_sol= Find_RMSE(x_newTrain, y_newTrain, x_Test,y_Test)\n",
        "\n",
        "print(\"\\nRoot Mean Square Error = \", new_RMSE)\n",
        "print('\\nVariable η_log is: ', new_η_log_sol,'\\nnVariable S_lin is: ', new_S_lin_sol,'\\nnVariable γ_log is: ', new_γ_log_sol)\n",
        "\n",
        "\n",
        "#Use the best γ, η and S found in the validation set for the stochastic gradient descent \n",
        "#display(TST_dataset)\n",
        "# Calculate the RMSE over the test data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v57FxmEnjPjj"
      },
      "source": [
        "# 2. Random forests (13 marks)\n",
        "\n",
        "**USE scikit-learn for the questions on this section.**\n",
        "\n",
        "In section 1, you used a regularised ridge regression model trained with SGD to create a linear predictive model. In this part of the assignment, you will use **scikit-learn** to train a random forest for regression over the air quality dataset.\n",
        "\n",
        "## 2.1 Preprocessing the data\n",
        "\n",
        "As mentioned before, the dataset has missing values tagged with a -200 value. Before doing any work with the training data, we want to make sure that we deal properly with the missing values. Furthermore, once we have dealt with the missing values, we want to standardise the training data. \n",
        "\n",
        "### Question 2.a: Pipeline for missing values and standardisation (3 marks)\n",
        "\n",
        "* Employ the `SimpleImputer` method in Scikit-learn to impute the missing values in each column using the mean value of the non-missing values, instead (**1 mark**).\n",
        "\n",
        "* Standardise the data by substracting the mean value for each feature and dividing the result by the standard deviation of each feature. Employ the `StandardScaler` method (**1 mark**).\n",
        "\n",
        "* Create a `Pipeline` that you can use to preprocess the data by filling missing values and then standardising the features (**1 mark**).\n",
        "\n",
        "#### Question 2.a Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihWIaj6xjPjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1649545b-efbb-42af-bd51-27d1efe37bbd"
      },
      "source": [
        "\n",
        "# Fit method calculates mean and std on whichever the dataset you choose,\n",
        "# and transform applies the transofrmation with the computed values by the fit.\n",
        "\n",
        "# Importing the SimpleImputer class from sklearn\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Initializing the SimpleImputer object with missing_value and strategy defined\n",
        "imp_mean = SimpleImputer(missing_values = -200, strategy='mean')\n",
        "\n",
        "# Fitting the SimpleImputer using the trainning data set\n",
        "imp_mean.fit(data_training_unproc)\n",
        "\n",
        "# Filling in the missing values in data_training_unproc using the fitted SimpleImputer\n",
        "si_train = imp_mean.transform(data_training_unproc)\n",
        "\n",
        "#Standardise the data by substracting the mean value for each feature and dividing the result by the standard deviation of each feature. \n",
        "#Employ the StandardScaler method.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        " \n",
        "\n",
        "# call estimator1.fit_transform(x_train) you compute mean and std on x_train_(and store them) and standarize _x_train. \n",
        "\n",
        "si_train_std=pd.DataFrame(scaler.fit_transform(si_train))\n",
        "\n",
        "# Name the columns\n",
        "si_train_std.columns=train_dataset.columns\n",
        "\n",
        "print('\\n', si_train_std)\n",
        "\n",
        "print(si_train_std is train_dataset)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pl_pipe = Pipeline([('imputer',SimpleImputer(missing_values = -200, strategy='mean')), ('scaler', StandardScaler())])\n",
        "\n",
        "# fit and then transform \n",
        "train_pipe = pd.DataFrame(pl_pipe.fit_transform(data_training_unproc))\n",
        "\n",
        "\n",
        "print('\\n', train_pipe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "         CO(GT)  PT08.S1(CO)  C6H6(GT)  ...         T        RH        AH\n",
            "0    -0.865286    -1.165745 -1.033054  ... -0.060006 -0.829670 -0.787753\n",
            "1    -0.933978    -1.006994 -0.570767  ...  2.261286 -1.947997  0.032378\n",
            "2    -0.178366    -0.287944  0.358452  ...  2.198239 -1.548908  1.026477\n",
            "3     0.989397     0.483634  0.720130  ... -1.128946 -0.594310 -1.461376\n",
            "4    -1.002670    -1.310489 -0.943156  ...  1.960378 -1.443653  0.895290\n",
            "...        ...          ...       ...  ...       ...       ...       ...\n",
            "5367 -1.140054    -0.933454 -1.201571  ... -0.641761  0.418763 -0.496865\n",
            "5368 -0.453134    -0.234248 -0.378508  ...  0.601992  0.965500  2.115698\n",
            "5369 -0.521826    -0.395334 -0.755739  ... -1.243578 -0.250771 -1.380211\n",
            "5370  0.027710    -0.353312 -0.089034  ...  0.229439  0.503551  0.828403\n",
            "5371 -0.521826    -0.119854 -0.709306  ... -0.610238  1.440606  0.179607\n",
            "\n",
            "[5372 rows x 12 columns]\n",
            "False\n",
            "\n",
            "             0         1         2   ...        9         10        11\n",
            "0    -0.865286 -1.165745 -1.033054  ... -0.060006 -0.829670 -0.787753\n",
            "1    -0.933978 -1.006994 -0.570767  ...  2.261286 -1.947997  0.032378\n",
            "2    -0.178366 -0.287944  0.358452  ...  2.198239 -1.548908  1.026477\n",
            "3     0.989397  0.483634  0.720130  ... -1.128946 -0.594310 -1.461376\n",
            "4    -1.002670 -1.310489 -0.943156  ...  1.960378 -1.443653  0.895290\n",
            "...        ...       ...       ...  ...       ...       ...       ...\n",
            "5367 -1.140054 -0.933454 -1.201571  ... -0.641761  0.418763 -0.496865\n",
            "5368 -0.453134 -0.234248 -0.378508  ...  0.601992  0.965500  2.115698\n",
            "5369 -0.521826 -0.395334 -0.755739  ... -1.243578 -0.250771 -1.380211\n",
            "5370  0.027710 -0.353312 -0.089034  ...  0.229439  0.503551  0.828403\n",
            "5371 -0.521826 -0.119854 -0.709306  ... -0.610238  1.440606  0.179607\n",
            "\n",
            "[5372 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoZsdlpDjPjk"
      },
      "source": [
        "### Question 2.b: Use the Pipeline to fit the training data and transform the validation data (2 marks)\n",
        "\n",
        "In the previous question, you created a `Pipeline` for applying a `SimpleImputer` and a `StandardScaler`. Use the Pipeline to fit the training data (**1 mark**) and transform the validation data (**1 mark**).\n",
        "\n",
        "#### Question 2.b Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edw7WEUdjPjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a0294d-5fa0-4e59-8865-b19ffb9f23f4"
      },
      "source": [
        "\n",
        "\n",
        "# Preprocess the validation data\n",
        "\n",
        "#Fit the trainning data into the pipeline\n",
        "pl_pipe.fit(data_training_unproc)\n",
        "\n",
        "#Transform the validation data with the pipeline\n",
        "Mixed_pipe = pd.DataFrame(pl_pipe.transform(data_val_unproc))\n",
        "\n",
        "Mixed_pipe.columns = data_val_unproc.columns\n",
        "print(Mixed_pipe)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        CO(GT)  PT08.S1(CO)  C6H6(GT)  ...         T        RH        AH\n",
            "0    -1.208746    -1.631493 -1.067871  ...  0.518884  0.088381  0.903319\n",
            "1    -0.590518    -0.598442 -0.868524  ... -0.604506  0.871941 -0.169113\n",
            "2    -0.315750    -0.428018 -0.173863  ...  0.914364 -1.738950 -0.911771\n",
            "3    -1.277438    -1.810088 -1.274367  ... -0.272074  0.230182 -0.176386\n",
            "4     1.332857     2.220560  1.506502  ... -0.753527  0.646814 -0.489080\n",
            "...        ...          ...       ...  ...       ...       ...       ...\n",
            "1146  2.019776     3.211589  1.986102  ...  0.152063 -0.455433 -0.220794\n",
            "1147  0.439861     0.503478  0.676349  ...  2.032023 -1.344247  1.261462\n",
            "1148  1.470241     1.640418  2.303466  ...  0.387058  0.990352  1.646772\n",
            "1149  1.538933     1.576217  1.252425  ...  0.157795  0.697979  0.888006\n",
            "1150  0.096401     1.025257  0.360483  ...  1.241064 -0.391111  1.639679\n",
            "\n",
            "[1151 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaG_ZWT2jPjl"
      },
      "source": [
        "## 2.2 Random forest to predict air quality \n",
        "\n",
        "We now use random forests to predict air quality. Remember that the tree ensemble in random forests is built by training individual regression trees on different subsets of the training data and using a subset of the available features. For regression, the prediction is the average of the individual predictions of each tree. Some of the parameters required in the Random Forest implementation in Scikit-learn include:\n",
        "\n",
        "Some of the additional parameters required in the Random Forest implementation in Scikit-learn include\n",
        "\n",
        "> **n_estimators** the total number of trees to train<p>\n",
        "**max_features** number of features to use as candidates for splitting at each tree node. <p>\n",
        "    **boostrap**: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.<p>\n",
        "   **max_samples**: If bootstrap is True, the number of samples to draw from X to train each base estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZE5SCgSjPjm"
      },
      "source": [
        "### Question 2.c: train a random forest (4 marks)\n",
        "\n",
        "In this question, you will train a random forest for predicting over the validation data. Use cross-validation over the validation data to select the best set of paramaters for the random forest regressor. Parameters to include in your exploration are **n_estimators**, **max_features** and **max_samples**. Use `np.linspace` or `np.logspace` to define ranges of values to explore for each parameter and create a grid to be assessed over the validation data. Make sure you use the same validation data that was given to you.\n",
        "\n",
        "* Use `PredefinedSplit` to tell the cross validator which instances to use for training and which ones for validation (**1 mark**).\n",
        "\n",
        "* Create a grid of values to explore that include a range of at least five values for each parameter **n_estimators**, **max_features** and **max_samples** (**1 mark**). \n",
        "\n",
        "* Train a random forest for regression model that uses the grid of parameters you created before. Use `GridSearchCV` to find the best set of parameters by performing cross-validation over the predefined split. (**1 mark**).\n",
        "\n",
        "* Print the best values in your grid for **n_estimators**, **max_features** and **max_samples** (**1 mark**).\n",
        "\n",
        "#### Question 2.c Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PmQNztbjPjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116df19a-1d23-47bb-c3df-06f5dac95586"
      },
      "source": [
        "from sklearn.model_selection import PredefinedSplit\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# AIR QUALITY\n",
        "# I need this to Train\n",
        "\n",
        "X_tr_rf= pd.DataFrame(train_pipe.iloc[:, 1:train_pipe.last_valid_index()])\n",
        "\n",
        "st_dev_1 = train_pipe.std()\n",
        "No_StanTrain = train_pipe*st_dev_1\n",
        "\n",
        "Y_tr_rf = pd.DataFrame(No_StanTrain.iloc[:, 0])\n",
        "\n",
        "\n",
        "Val_pipe = pd.DataFrame(pl_pipe.fit_transform(data_val_unproc))\n",
        "\n",
        "X_Val_rf = pd.DataFrame(Val_pipe.iloc[:, 1:Val_pipe.last_valid_index()])\n",
        "st_dev_2 = Val_pipe.std()\n",
        "\n",
        "#For yVal we need to reverse the standardization, otherwise it will affect the computation of w, using minibatch gradient descent.\n",
        "No_StanVal = Val_pipe*st_dev_2\n",
        "\n",
        "#Validation Dataset separated into the vector of targets y, called yTrain. take the first column CO(GT)\n",
        "y_Val_rf = pd.DataFrame(No_StanVal.iloc[:, 0])\n",
        "\n",
        "\n",
        "# Create a grid of values to explore that include a range of at least five values for each parameter n_estimators, max_features and max_samples (1 mark).\n",
        "\n",
        "# n_estimators the total number of trees to train maybe try small\n",
        "n_estimators = np.linspace(50.0,\n",
        "               150.0,\n",
        "               num = 5,\n",
        "               endpoint = False,\n",
        "               retstep = False,\n",
        "               dtype = int)\n",
        "print(\"n_estimators\\n\", n_estimators,'\\n')\n",
        "\n",
        "# max_features number of features to use as candidates for splitting at each tree node.\n",
        "\n",
        "# max_features = ['auto', 'sqrt']\n",
        "max_features = np.logspace(1.0,\n",
        "               4.0,\n",
        "               num = 5,\n",
        "               endpoint = False,\n",
        "               base = 2.0,\n",
        "               dtype = int)\n",
        "print(\"max_features\\n\", max_features,'\\n')\n",
        "\n",
        " # max_samples: If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
        "\n",
        "max_samples = np.logspace(2.0,\n",
        "               5.0,\n",
        "               num = 5,\n",
        "               endpoint = False,\n",
        "               base = 4.0,\n",
        "               dtype = int)\n",
        "print(\"max_samples\\n\", max_samples,'\\n')\n",
        "\n",
        " # boostrap: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "param_grid = { \n",
        "    'n_estimators': n_estimators,\n",
        "    'max_features': max_features,\n",
        "    'max_samples' : max_samples,\n",
        "    'bootstrap' : [True,False]\n",
        "}\n",
        "\n",
        "# Use PredefinedSplit to tell the cross validator which instances to use for training and which ones for validation (1 mark).\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "index = [0 if x in X_Val_rf.index else -1 for x in data_val_unproc.index] # HOW TO DO THIS\n",
        "\n",
        "split = PredefinedSplit(index)#SPLIT WHAT\n",
        "\n",
        "Pre_Split = PredefinedSplit(test_fold = index)\n",
        "# print(Pre_Split.get_n_splits())\n",
        "\n",
        "validation_split = list(PredefinedSplit(index).split())\n",
        "\n",
        "# Set up GridSearch with RandomForest as an estimator, load the paramater grid ......\n",
        "CV_rfc = GridSearchCV(estimator=rf, param_grid=param_grid, cv = validation_split, refit = True)\n",
        "\n",
        "# train the model on train set using GridSearchCV \n",
        "CV_rfc.fit(X_tr_rf, Y_tr_rf.values.ravel())\n",
        "\n",
        "best_parameters = CV_rfc.best_params_\n",
        "print(\"Best values in the grid:\",best_parameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_estimators\n",
            " [ 50  70  90 110 130] \n",
            "\n",
            "max_features\n",
            " [ 2  3  4  6 10] \n",
            "\n",
            "max_samples\n",
            " [ 16  36  84 194 445] \n",
            "\n",
            "Best values in the grid: {'bootstrap': False, 'max_features': 3, 'max_samples': 16, 'n_estimators': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebNvRHrUjPjm"
      },
      "source": [
        "### Question 2.d: train a new model over the whole training data and report the prediction over the test set (4 marks)\n",
        "\n",
        "\n",
        "Now that we have identified the best paramaters of the regression model, we use these parameters to train a new model over the whole training data (`data_training` plus `data_val`). We apply this model to the test set and report the performance.\n",
        "\n",
        "* Create a new preprocessing pipeline for taking care of the missing values and standardisation over the whole training data (**1 mark**).\n",
        "\n",
        "* Apply the created preprocessing pipeline to the test data (**1 mark**).\n",
        "\n",
        "* Fit a random forest regression model to the training data using the best parameters found in Question 2.c (**1 mark**).\n",
        "\n",
        "* Compute the RMSE over the test data and report the result (**1 mark**).\n",
        "\n",
        "#### Question 2.d Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jgnEMoIjPjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3af6f50-4306-4ada-80e7-822ab4f019ae"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "new_dataTR = [data_val_unproc, data_training_unproc]\n",
        "  \n",
        "new_unproc_TR = pd.concat(new_dataTR)\n",
        "\n",
        "#Similar to 2c TEST for VAL and Train \n",
        "#Create a new preprocessing pipeline for taking care of the missing values and standardisation over the whole training data \n",
        "fn_pipe = Pipeline([('imputer',SimpleImputer(missing_values = -200, strategy = 'mean')), ('scaler', StandardScaler())])\n",
        "\n",
        "new_TR_pipe = pd.DataFrame(fn_pipe.fit_transform(new_unproc_TR))# fit and then transform \n",
        "\n",
        "new_TR_pipe.columns = new_unproc_TR.columns\n",
        "\n",
        "X_new_rf = pd.DataFrame(new_TR_pipe.iloc[:, 1:new_TR_pipe.last_valid_index()])\n",
        "\n",
        "st_dev_new = new_TR_pipe.std()\n",
        "New_StanTrain = new_TR_pipe*st_dev_new\n",
        "\n",
        "Y_new_rf = pd.DataFrame(New_StanTrain.iloc[:, 0])\n",
        "\n",
        "\n",
        "# Apply the created preprocessing pipeline to the test data \n",
        "test_pipe = pd.DataFrame(fn_pipe.fit_transform(data_test_unproc))\n",
        "\n",
        "test_pipe.columns = data_test_unproc.columns\n",
        "\n",
        "\n",
        "x_TestRF = pd.DataFrame(test_pipe.iloc[:, 1:test_pipe.last_valid_index()])\n",
        "\n",
        "st_devTST = test_pipe.std()\n",
        "New_StanTST = test_pipe*st_devTST\n",
        "\n",
        "y_TestRF = pd.DataFrame(New_StanTST.iloc[:, 0])\n",
        "\n",
        "#fit a random forest regression model to the training data using the best parameters found in Question 2.c \n",
        "rf1 = RandomForestRegressor()\n",
        "\n",
        "list_param ={\n",
        "    'n_estimators': np.array([best_parameters.get('n_estimators')]),\n",
        "    'max_features': np.array([best_parameters.get('max_features')]),\n",
        "    'max_samples' : np.array([best_parameters.get('max_samples')]),\n",
        "    'bootstrap' :  np.array([best_parameters.get('bootstrap')])\n",
        "}\n",
        "\n",
        "\n",
        "# Set up GridSearch with RandomForest as an estimator, load the paramater grid ......\n",
        "CV_rfc1 = GridSearchCV(estimator=rf1, param_grid = list_param,  refit = True)\n",
        "\n",
        "# train the model on train set using GridSearchCV \n",
        "CV_rfc1.fit(X_new_rf, Y_new_rf.values.ravel())\n",
        "\n",
        "# RMSE over the test data\n",
        "y_pred = CV_rfc1.predict(x_TestRF)\n",
        "\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_TestRF, y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error: 0.2972839266882645\n"
          ]
        }
      ]
    }
  ]
}